---
title: "최대우도추정법의 이론과 실무 적용"
date: "2025-01-30"
category: "statistics"
excerpt: "최대우도추정법(MLE)의 수학적 이론을 이해하고, 보험업계에서의 실제 적용 사례를 통해 실무 역량을 높여보겠습니다."
tags: ["MLE", "최대우도추정", "통계학"]
---

최대우도추정법(Maximum Likelihood Estimation, MLE)은 통계학에서 가장 중요한 추정 방법 중 하나입니다. 이번 포스트에서는 MLE의 이론적 배경과 보험업계에서의 실무 적용을 자세히 살펴보겠습니다.

## MLE의 기본 개념

### 우도함수 (Likelihood Function)
우도함수는 주어진 데이터가 관측될 확률을 매개변수의 함수로 나타낸 것입니다.

**정의:**
```
L(θ) = ∏(i=1 to n) f(xᵢ|θ)
```

여기서:
- θ: 추정하고자 하는 매개변수
- xᵢ: i번째 관측값
- f(xᵢ|θ): 확률밀도함수

### 로그우도함수
계산의 편의를 위해 로그우도함수를 사용합니다:

```
ℓ(θ) = ln L(θ) = ∑(i=1 to n) ln f(xᵢ|θ)
```

### MLE 원리
MLE는 우도함수를 최대화하는 매개변수 θ̂를 찾는 방법입니다:

```
θ̂ = argmax L(θ) = argmax ℓ(θ)
```

## 수학적 유도 과정

### 1. 정규분포의 경우
정규분포 N(μ, σ²)에서 MLE를 유도해보겠습니다.

**로그우도함수:**
```
ℓ(μ,σ²) = -n/2 ln(2π) - n/2 ln(σ²) - 1/(2σ²) ∑(xᵢ - μ)²
```

**1차 조건:**
```
∂ℓ/∂μ = 1/σ² ∑(xᵢ - μ) = 0
∂ℓ/∂σ² = -n/(2σ²) + 1/(2σ⁴) ∑(xᵢ - μ)² = 0
```

**MLE 추정량:**
```
μ̂ = (1/n) ∑xᵢ = x̄
σ̂² = (1/n) ∑(xᵢ - x̄)²
```

### 2. 감마분포의 경우
감마분포 Γ(α, β)에서 MLE를 유도해보겠습니다.

**로그우도함수:**
```
ℓ(α,β) = nα ln β - n ln Γ(α) + (α-1)∑ln xᵢ - β∑xᵢ
```

**1차 조건:**
```
∂ℓ/∂α = n ln β - n ψ(α) + ∑ln xᵢ = 0
∂ℓ/∂β = nα/β - ∑xᵢ = 0
```

여기서 ψ(α)는 디감마 함수입니다.

## R을 활용한 MLE 구현

### 1. 기본 MLE 함수
```r
# 일반적인 MLE 함수
mle_optimization <- function(data, distribution, initial_params) {
  # 로그우도함수 정의
  log_likelihood <- function(params) {
    if (distribution == "normal") {
      mu <- params[1]
      sigma <- params[2]
      if (sigma <= 0) return(-Inf)
      sum(dnorm(data, mu, sigma, log = TRUE))
    } else if (distribution == "gamma") {
      alpha <- params[1]
      beta <- params[2]
      if (alpha <= 0 || beta <= 0) return(-Inf)
      sum(dgamma(data, alpha, rate = beta, log = TRUE))
    }
  }
  
  # 최적화
  result <- optim(initial_params, 
                  function(x) -log_likelihood(x),
                  method = "L-BFGS-B",
                  lower = c(-Inf, 0.001))
  
  return(list(
    params = result$par,
    log_likelihood = -result$value,
    convergence = result$convergence
  ))
}
```

### 2. 정규분포 MLE
```r
# 정규분포 MLE
normal_mle <- function(data) {
  n <- length(data)
  mu_init <- mean(data)
  sigma_init <- sd(data)
  
  result <- mle_optimization(data, "normal", c(mu_init, sigma_init))
  
  return(list(
    mu = result$params[1],
    sigma = result$params[2],
    log_likelihood = result$log_likelihood
  ))
}

# 예제
set.seed(123)
normal_data <- rnorm(1000, mean = 100, sd = 15)
normal_result <- normal_mle(normal_data)
print(normal_result)
```

### 3. 감마분포 MLE
```r
# 감마분포 MLE
gamma_mle <- function(data) {
  # 초기값 계산
  alpha_init <- mean(data)^2 / var(data)
  beta_init <- var(data) / mean(data)
  
  result <- mle_optimization(data, "gamma", c(alpha_init, beta_init))
  
  return(list(
    alpha = result$params[1],
    beta = result$params[2],
    log_likelihood = result$log_likelihood
  ))
}

# 예제
set.seed(123)
gamma_data <- rgamma(1000, shape = 2.5, rate = 0.01)
gamma_result <- gamma_mle(gamma_data)
print(gamma_result)
```

## Python을 활용한 MLE 구현

### 1. Scipy를 활용한 MLE
```python
import numpy as np
import scipy.stats as stats
from scipy.optimize import minimize
import matplotlib.pyplot as plt

def mle_normal(data):
    """정규분포 MLE"""
    n = len(data)
    
    def neg_log_likelihood(params):
        mu, sigma = params
        if sigma <= 0:
            return np.inf
        return -np.sum(stats.norm.logpdf(data, mu, sigma))
    
    # 초기값
    mu_init = np.mean(data)
    sigma_init = np.std(data)
    
    # 최적화
    result = minimize(neg_log_likelihood, [mu_init, sigma_init], 
                      method='L-BFGS-B', bounds=[(None, None), (0.001, None)])
    
    return {
        'mu': result.x[0],
        'sigma': result.x[1],
        'log_likelihood': -result.fun
    }

# 예제
np.random.seed(123)
normal_data = np.random.normal(100, 15, 1000)
normal_result = mle_normal(normal_data)
print(f"추정된 μ: {normal_result['mu']:.4f}")
print(f"추정된 σ: {normal_result['sigma']:.4f}")
```

### 2. 감마분포 MLE
```python
def mle_gamma(data):
    """감마분포 MLE"""
    n = len(data)
    
    def neg_log_likelihood(params):
        alpha, beta = params
        if alpha <= 0 or beta <= 0:
            return np.inf
        return -np.sum(stats.gamma.logpdf(data, alpha, scale=1/beta))
    
    # 초기값
    alpha_init = np.mean(data)**2 / np.var(data)
    beta_init = np.var(data) / np.mean(data)
    
    # 최적화
    result = minimize(neg_log_likelihood, [alpha_init, beta_init], 
                      method='L-BFGS-B', bounds=[(0.001, None), (0.001, None)])
    
    return {
        'alpha': result.x[0],
        'beta': result.x[1],
        'log_likelihood': -result.fun
    }

# 예제
np.random.seed(123)
gamma_data = np.random.gamma(2.5, 100, 1000)
gamma_result = mle_gamma(gamma_data)
print(f"추정된 α: {gamma_result['alpha']:.4f}")
print(f"추정된 β: {gamma_result['beta']:.4f}")
```

## 보험업계 실무 적용

### 1. 손해액 분포 모델링
```python
def insurance_loss_modeling(loss_data):
    """보험 손해액 분포 모델링"""
    
    # 여러 분포 비교
    distributions = {
        'Normal': stats.norm,
        'Gamma': stats.gamma,
        'Lognormal': stats.lognorm,
        'Weibull': stats.weibull_min
    }
    
    results = {}
    for name, dist in distributions.items():
        try:
            if name == 'Normal':
                params = dist.fit(loss_data)
                log_likelihood = np.sum(dist.logpdf(loss_data, *params))
            elif name == 'Lognormal':
                params = dist.fit(loss_data, floc=0)
                log_likelihood = np.sum(dist.logpdf(loss_data, *params))
            else:
                params = dist.fit(loss_data, floc=0)
                log_likelihood = np.sum(dist.logpdf(loss_data, *params))
            
            # AIC 계산
            aic = -2 * log_likelihood + 2 * len(params)
            
            results[name] = {
                'params': params,
                'log_likelihood': log_likelihood,
                'aic': aic
            }
        except:
            continue
    
    # 최적 모델 선택
    best_model = min(results.keys(), key=lambda x: results[x]['aic'])
    
    return results, best_model

# 보험 손해 데이터 예제
np.random.seed(123)
loss_data = np.random.gamma(2.0, 50000, 1000)  # 평균 100,000원 손해

results, best_model = insurance_loss_modeling(loss_data)
print(f"최적 모델: {best_model}")
print(f"AIC: {results[best_model]['aic']:.2f}")
```

### 2. VaR 및 CVaR 계산
```python
def calculate_risk_metrics(loss_data, confidence_levels=[0.95, 0.99]):
    """리스크 지표 계산"""
    
    # 최적 분포 선택
    results, best_model = insurance_loss_modeling(loss_data)
    best_params = results[best_model]['params']
    
    risk_metrics = {}
    
    for cl in confidence_levels:
        if best_model == 'Gamma':
            var = stats.gamma.ppf(cl, *best_params)
            # CVaR 계산
            def integrand(x):
                return x * stats.gamma.pdf(x, *best_params)
            from scipy.integrate import quad
            numerator, _ = quad(integrand, var, np.inf)
            cvar = numerator / (1 - cl)
        elif best_model == 'Lognormal':
            var = stats.lognorm.ppf(cl, *best_params)
            # CVaR 계산은 유사하게...
        
        risk_metrics[f'VaR_{int(cl*100)}'] = var
        risk_metrics[f'CVaR_{int(cl*100)}'] = cvar
    
    return risk_metrics, best_model

# 리스크 지표 계산
risk_metrics, model = calculate_risk_metrics(loss_data)
print("리스크 지표:")
for metric, value in risk_metrics.items():
    print(f"{metric}: {value:,.0f}원")
```

### 3. 신뢰구간 계산
```python
def mle_confidence_interval(data, distribution, confidence_level=0.95):
    """MLE 신뢰구간 계산"""
    
    # MLE 추정
    if distribution == 'gamma':
        result = mle_gamma(data)
        alpha, beta = result['alpha'], result['beta']
        
        # Fisher 정보행렬 계산 (근사)
        n = len(data)
        # 2차 도함수 근사
        hessian_approx = np.array([
            [n * stats.polygamma(1, alpha), 0],
            [0, n * alpha / (beta**2)]
        ])
        
        # 공분산 행렬
        cov_matrix = np.linalg.inv(hessian_approx)
        
        # 표준오차
        se_alpha = np.sqrt(cov_matrix[0, 0])
        se_beta = np.sqrt(cov_matrix[1, 1])
        
        # 신뢰구간
        z_score = stats.norm.ppf(1 - (1 - confidence_level) / 2)
        
        ci_alpha = (alpha - z_score * se_alpha, alpha + z_score * se_alpha)
        ci_beta = (beta - z_score * se_beta, beta + z_score * se_beta)
        
        return {
            'alpha': (alpha, ci_alpha),
            'beta': (beta, ci_beta)
        }

# 신뢰구간 계산
ci_result = mle_confidence_interval(loss_data, 'gamma')
print("95% 신뢰구간:")
print(f"α: {ci_result['alpha'][0]:.4f} [{ci_result['alpha'][1][0]:.4f}, {ci_result['alpha'][1][1]:.4f}]")
print(f"β: {ci_result['beta'][0]:.4f} [{ci_result['beta'][1][0]:.4f}, {ci_result['beta'][1][1]:.4f}]")
```

## 고급 주제

### 1. 베이지안 MLE
```python
import pymc3 as pm
import theano.tensor as tt

def bayesian_mle(data, distribution='gamma'):
    """베이지안 MLE"""
    
    with pm.Model() as model:
        if distribution == 'gamma':
            # 사전분포
            alpha = pm.Gamma('alpha', alpha=1, beta=1)
            beta = pm.Gamma('beta', alpha=1, beta=1)
            
            # 우도
            likelihood = pm.Gamma('likelihood', alpha=alpha, beta=beta, observed=data)
        
        # MCMC 샘플링
        trace = pm.sample(2000, tune=1000, cores=2)
    
    return trace

# 베이지안 MLE
trace = bayesian_mle(loss_data)
pm.plot_trace(trace)
pm.summary(trace)
```

### 2. 모델 선택
```python
def model_selection(data, distributions=['normal', 'gamma', 'lognormal']):
    """모델 선택"""
    
    results = {}
    
    for dist in distributions:
        if dist == 'normal':
            params = stats.norm.fit(data)
            log_likelihood = np.sum(stats.norm.logpdf(data, *params))
            k = 2  # 매개변수 개수
        elif dist == 'gamma':
            params = stats.gamma.fit(data, floc=0)
            log_likelihood = np.sum(stats.gamma.logpdf(data, *params))
            k = 2
        elif dist == 'lognormal':
            params = stats.lognorm.fit(data, floc=0)
            log_likelihood = np.sum(stats.lognorm.logpdf(data, *params))
            k = 2
        
        # 정보기준
        aic = -2 * log_likelihood + 2 * k
        bic = -2 * log_likelihood + k * np.log(len(data))
        
        results[dist] = {
            'params': params,
            'log_likelihood': log_likelihood,
            'aic': aic,
            'bic': bic
        }
    
    return results

# 모델 선택
model_results = model_selection(loss_data)
print("모델 비교:")
for model, result in model_results.items():
    print(f"{model}: AIC={result['aic']:.2f}, BIC={result['bic']:.2f}")
```

## 실무 적용 팁

### 1. 수렴성 확인
```python
def check_convergence(data, distribution, max_iter=1000):
    """MLE 수렴성 확인"""
    
    # 여러 초기값으로 시도
    initial_values = [
        [np.mean(data)**2 / np.var(data), np.var(data) / np.mean(data)],
        [1.0, 1.0],
        [5.0, 0.1]
    ]
    
    results = []
    for init in initial_values:
        try:
            if distribution == 'gamma':
                result = mle_gamma(data)
                results.append(result)
        except:
            continue
    
    # 결과 비교
    if len(results) > 1:
        alpha_values = [r['alpha'] for r in results]
        beta_values = [r['beta'] for r in results]
        
        print(f"α 추정값 범위: {min(alpha_values):.4f} - {max(alpha_values):.4f}")
        print(f"β 추정값 범위: {min(beta_values):.4f} - {max(beta_values):.4f}")
    
    return results
```

### 2. 적합도 검정
```python
def goodness_of_fit(data, distribution, params):
    """적합도 검정"""
    
    if distribution == 'gamma':
        # Kolmogorov-Smirnov 검정
        ks_stat, ks_pvalue = stats.kstest(data, lambda x: stats.gamma.cdf(x, *params))
        
        # Anderson-Darling 검정
        ad_stat, ad_critical, ad_significance = stats.anderson(data, dist='gamma')
        
        return {
            'ks_statistic': ks_stat,
            'ks_pvalue': ks_pvalue,
            'ad_statistic': ad_stat,
            'ad_critical': ad_critical
        }

# 적합도 검정
fit_test = goodness_of_fit(loss_data, 'gamma', [2.0, 0.02])
print("적합도 검정 결과:")
print(f"KS 통계량: {fit_test['ks_statistic']:.4f}")
print(f"KS p-value: {fit_test['ks_pvalue']:.4f}")
```

## 결론

최대우도추정법은 보험업계에서 분포 모델링의 핵심 도구입니다. [Real Statistics의 MLE 방법론](https://real-statistics.com/distribution-fitting/distribution-fitting-via-maximum-likelihood/fitting-gamma-parameters-mle/)을 기반으로 한 체계적인 접근을 통해 정확한 리스크 측정과 보험료 산출이 가능합니다.

> "MLE는 통계학의 핵심이며, 보험업계에서의 올바른 적용은 정확한 리스크 관리의 기초가 됩니다."

앞으로 더 복잡한 다변량 분포와 시계열 모델링에 대해 다루어보겠습니다.
